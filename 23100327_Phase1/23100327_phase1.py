# -*- coding: utf-8 -*-
"""23100327_Phase1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8lri2oEOzAYsLeJu-uumnBgexDjusHE
"""

!pip install -q snscrape==0.3.4

# ***** TASK 1 *****
# Scraping tweets using SNSSCRAPE
# Working from: https://www.freecodecamp.org/news/python-web-scraping-tutorial/
import snscrape.modules.twitter as sntwitter
import pandas as pd
import numpy as np

tweetsCollected = []

for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:@AlboMP').get_items()):
    if i>1000:
        break
    tweetsCollected.append([tweet.content])

print("First 10 Tweets Extracted:")
for i in range(10):
  print(tweetsCollected[i])  

# tweetsCollected = np.array(tweetsCollected)
 
# tweetsCollected.to_numpy()
# np.savetxt('sample.csv',tweetsCollected, delimiter=",")    

# tweetsCollected.tofile('sample.csv', sep = ',')    

dataFrame = pd.DataFrame(tweetsCollected)
dataFrame.to_csv("AlboMP_task1.csv", index = False, header = False)

# ***** TASK 2 *****
import regex as re
import pandas as pd

# Clean Data
df = pd.read_csv('/content/drive/My Drive/AlboMP_task1.csv')
# newdf = df.drop(, axis=1)
# print(df)
tweetArray = df.to_numpy()

# print(tweetArray[0])
cleanArray = []

# remove punctations
for x in tweetArray:
  # remove html
  cleanData = re.sub('https://[\S]+', "", x[0])
  cleanData = re.sub('http://[\S]+', "", cleanData)

  # remove hashtags
  cleanData = re.sub("#[A-Za-z0-9_]+","", cleanData)

  # remove mentions
  cleanData = re.sub("@[A-Za-z0-9_]+","", cleanData)

  # convert to lower case
  cleanData = cleanData.lower()

  # Remove Punctuations
  cleanData = re.sub("[0-9 , : ; % \ / ! * $ • ` ” “ ? ~ ^ = -  ’ @ # ( ) ° . - ; ″ '  – \ - \"]", "", cleanData)
  
  # Remove Emojis
  cleanData = re.sub("[^a-z]","", cleanData)
  cleanArray.append(cleanData)

# Save cleaned tweets in csv
# print(cleanArray)
dataFrame = pd.DataFrame(cleanArray)
dataFrame.to_csv("AlboMP_task2.csv", index = False, header = False)

# ***** TASK 3 *****
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/My Drive/AlboMP_task2.csv')
df = df.dropna()
tweetArray = df.to_numpy()
X = tweetArray
X_train, X_test = train_test_split(X, test_size=0.20, random_state = 50)

def bagOfWords(splitSentence, vocab):
  # print(splitSentence)
  vectorFeature = []
  for x in vocab:
    freq = splitSentence.count(x)
    vectorFeature.append(freq)
  return vectorFeature

# Laplace Smoothing
def smoothing(vecorBOW):
  smoothList = []
  for x in vecorBOW:
    smoothList.append([i+1 for i in x])
  print("\n")
  print("Smooth Data")
  for i in range(10):
    print(smoothList[i])

def createVocab():
  vocab = []
  for x in X_train:
    splitWords = x[0].split()
    # print(splitWords)
    for i in splitWords:
      if i not in vocab:
        vocab.append(i)
        
  return vocab

# Print Vocabulary
vocab = createVocab()
print("Vocabulary:", vocab)

# For Train Data
vectors = []
for x in X_train:
  splitWords = x[0].split()
  result = bagOfWords(splitWords, vocab)
  # break
  vectors.append(result)

print("Bag of words - Train Data")
for i in range(10):
  print(vectors[i])

smoothing(vectors)

# For Test Data
vectorTest = []
for x in X_test:
  splitWords = x[0].split()
  result = bagOfWords(splitWords, vocab)
  # break
  vectorTest.append(result)

print("Bag of words - Test Data")
for i in range(10):
  print(vectorTest[i])

smoothing(vectorTest)